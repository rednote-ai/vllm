# Returning Multimodal Embeddings

This feature allows you to retrieve the multimodal embeddings generated by the encoder during inference. This is useful for extracting intermediate representations from multimodal models.

## Overview

When processing multimodal inputs (e.g., images, audio, video) with a multimodal model, the encoder generates embeddings that are then fed into the language model. With `enable_return_mm_embedding=True`, these embeddings are captured and returned as part of the output.

## Usage

### With LLM Class

```python
from vllm import LLM, SamplingParams
from PIL import Image

# Initialize LLM with enable_return_mm_embedding=True
llm = LLM(
    model="Qwen/Qwen3-VL-4B-Instruct",
    enable_return_mm_embedding=True,
    max_model_len=4096,
    limit_mm_per_prompt={"image": 1},
)

# Prepare multimodal input
image = Image.open("https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/boxes.png")
prompt = (
    "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
    "<|im_start|>user\n"
    "<|vision_start|><|image_pad|><|vision_end|>"
    "Describe this image.<|im_end|>\n"
    "<|im_start|>assistant\n"
)

# Generate
sampling_params = SamplingParams(temperature=0.0, max_tokens=100)
outputs = llm.generate(
    {
        "prompt": prompt,
        "multi_modal_data": {"image": image},
    },
    sampling_params=sampling_params,
)

# Access the multimodal embedding
for output in outputs:
    for completion in output.outputs:
        if completion.mm_embedding is not None:
            print(f"MM Embedding shape: {completion.mm_embedding.shape}")
            print(f"MM Embedding device: {completion.mm_embedding.device}")
            # The embedding is a torch.Tensor on CPU
            # Shape: (num_features, hidden_size)
```

### With vLLM Server

Start the server with the `--enable-return-mm-embedding` flag:

```bash
vllm serve Qwen/Qwen3-VL-4B-Instruct \
    --enable-return-mm-embedding \
    --limit-mm-per-prompt image=1
```

Note: The OpenAI-compatible API does not currently expose multimodal embeddings in the response. This feature is primarily designed for use with the LLM class and AsyncEngine.

## Key Features

1. **Automatic Capture**: When enabled, multimodal embeddings are automatically captured during encoder execution.

2. **CPU Tensors**: Embeddings are returned as PyTorch tensors on CPU to avoid GPU memory overhead.

3. **Request-Level**: Each request gets its own multimodal embedding, which is returned when the request completes.

4. **Streaming Compatible**: Works with both streaming and non-streaming outputs.

5. **Text-Only Handling**: For text-only inputs (no multimodal data), `mm_embedding` will be `None`.

## Implementation Details

### Data Flow

1. **Encoder Execution**: When the model processes multimodal inputs, the encoder generates embeddings.

2. **Capture**: The `MMEmbeddingCapturer` captures these embeddings and stores them per request.

3. **Retrieval**: When the request completes, the scheduler retrieves the embedding.

4. **Output**: The embedding is included in the `CompletionOutput` as `mm_embedding`.

### Memory Considerations

- Embeddings are moved to CPU immediately after capture to minimize GPU memory usage.
- Embeddings are automatically cleaned up when requests complete.
- For long-running requests or large batches, monitor CPU memory usage.

## Example Use Cases

1. **Feature Extraction**: Extract multimodal features for downstream tasks.

2. **Embedding Analysis**: Analyze how the model represents different modalities.

3. **Transfer Learning**: Use the embeddings as input to other models.

4. **Debugging**: Inspect intermediate representations during model development.

## Limitations

1. **Multimodal Models Only**: This feature only works with models that have multimodal encoders.

2. **Single Modality Per Request**: Currently optimized for requests with a single multimodal input.

3. **Memory Overhead**: Storing embeddings on CPU adds memory overhead proportional to the number of active requests.

## Configuration

The feature is controlled by the `enable_return_mm_embedding` parameter in `MultiModalConfig`:

```python
from vllm.config import MultiModalConfig

mm_config = MultiModalConfig(
    enable_return_mm_embedding=True,
    # ... other multimodal config options
)
```

## Related Features

- **Routed Experts**: Similar feature for returning expert routing decisions in MoE models.
- **Prompt Embeddings**: For passing pre-computed embeddings as input.

## Troubleshooting

**Q: The `mm_embedding` is always `None`.**

A: Check that:
- `enable_return_mm_embedding=True` is set
- Your input includes multimodal data (not just text)
- The model is a multimodal model with an encoder

**Q: High CPU memory usage.**

A: The embeddings are stored on CPU. For large batches or long sequences, this can consume significant memory. Consider:
- Reducing batch size
- Processing requests in smaller groups
- Extracting and saving embeddings immediately after generation

**Q: Does this work with streaming?**

A: Yes, the embedding is returned when the request completes (when `finish_reason` is set).
