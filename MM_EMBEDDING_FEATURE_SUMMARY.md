# Multimodal Embedding Return Feature - Implementation Summary

## Overview

This feature enables vLLM to return multimodal embeddings generated by the encoder during inference. When `enable_return_mm_embedding=True`, the embeddings are captured and returned as torch CPU tensors in the `CompletionOutput`.

## Implementation Details

### 1. Configuration Changes

#### [`vllm/config/multimodal.py`](vllm/config/multimodal.py)
- Added `enable_return_mm_embedding: bool = False` parameter to `MultiModalConfig`
- Documentation explains the feature and its use case

#### [`vllm/engine/arg_utils.py`](vllm/engine/arg_utils.py)
- Added `--enable-return-mm-embedding` CLI argument in the multimodal group
- Automatically extracted from `MultiModalConfig` using `get_kwargs()`

#### [`vllm/entrypoints/llm.py`](vllm/entrypoints/llm.py)
- Added `enable_return_mm_embedding: bool = False` parameter to `LLM.__init__()`
- Added documentation for the parameter
- Passed to `EngineArgs` for engine initialization

### 2. Output Data Structure Changes

#### [`vllm/outputs.py`](vllm/outputs.py)
- Added `mm_embedding: torch.Tensor | None = None` field to `CompletionOutput`
- Updated `__repr__()` to include mm_embedding
- Added documentation for the new field

#### [`vllm/v1/engine/__init__.py`](vllm/v1/engine/__init__.py)
- Added `mm_embedding: "torch.Tensor | None" = None` field to `EngineCoreOutput`

### 3. Embedding Capture Logic

#### [`vllm/multimodal/mm_embedding_capturer.py`](vllm/multimodal/mm_embedding_capturer.py) (NEW FILE)
- Created `MMEmbeddingCapturer` class (similar to `RoutedExpertsCapturer`)
- Singleton pattern for global instance management
- Methods:
  - `create()`: Create global singleton instance
  - `get_instance()`: Get the global instance
  - `capture(request_id, encoder_outputs)`: Capture embeddings for a request
  - `get_embedding(request_id)`: Retrieve captured embedding
  - `remove_embedding(request_id)`: Remove embedding for a request
  - `clear()`: Clear all embeddings
  - `cleanup()`: Clean up resources

Key differences from `RoutedExpertsCapturer`:
- No shared memory (simpler, request-level storage)
- No layer-by-layer capture (single capture per request)
- Stores concatenated encoder outputs as CPU tensors

#### [`vllm/v1/worker/gpu_model_runner.py`](vllm/v1/worker/gpu_model_runner.py)
- Added import for `MMEmbeddingCapturer`
- Added `init_mm_embedding_capturer()` method
- Called in `initialize_kv_cache()` to initialize the capturer
- Modified `_execute_mm_encoder()` to capture embeddings:
  - After caching encoder outputs
  - Maps encoder outputs to request IDs
  - Calls `mm_embedding_capturer.capture()` for each request

### 4. Scheduler Integration

#### [`vllm/v1/core/sched/scheduler.py`](vllm/v1/core/sched/scheduler.py)
- Added `_get_mm_embedding(request)` method:
  - Checks if feature is enabled
  - Retrieves embedding from `MMEmbeddingCapturer`
  - Returns `torch.Tensor | None`
- Modified `update_from_output()`:
  - Calls `_get_mm_embedding()` when request stops
  - Passes `mm_embedding` to `EngineCoreOutput`

### 5. Output Processing

#### [`vllm/v1/engine/output_processor.py`](vllm/v1/engine/output_processor.py)
- Modified `RequestState.make_request_output()`:
  - Added `mm_embedding` parameter
  - Passed to `_new_completion_output()`
- Modified `RequestState._new_completion_output()`:
  - Added `mm_embedding` parameter
  - Included in `CompletionOutput` construction
- Modified `OutputProcessor.process_outputs()`:
  - Extracts `mm_embedding` from `engine_core_output`
  - Passes to `req_state.make_request_output()`

## Data Flow

```
User Request (with multimodal data)
    ↓
LLM.generate(enable_return_mm_embedding=True)
    ↓
GPUModelRunner._execute_mm_encoder()
    ├─ model.embed_multimodal() → encoder_outputs
    └─ MMEmbeddingCapturer.capture(request_id, encoder_outputs)
        └─ Stores concatenated embeddings on CPU
    ↓
Scheduler.update_from_output()
    └─ _get_mm_embedding(request)
        └─ MMEmbeddingCapturer.get_embedding(request_id)
    ↓
EngineCoreOutput(mm_embedding=...)
    ↓
OutputProcessor.process_outputs()
    └─ RequestState.make_request_output(mm_embedding=...)
        └─ CompletionOutput(mm_embedding=...)
    ↓
User receives RequestOutput with mm_embedding
```

## Testing

### Unit Tests

#### [`tests/multimodal/test_mm_embedding_capture.py`](tests/multimodal/test_mm_embedding_capture.py) (NEW FILE)
- `test_mm_embedding_capturer_create()`: Test singleton creation
- `test_mm_embedding_capturer_capture_and_get()`: Test capture and retrieval
- `test_mm_embedding_capturer_remove()`: Test removal
- `test_mm_embedding_capturer_clear()`: Test clearing all embeddings
- `test_mm_embedding_capturer_empty_outputs()`: Test empty outputs handling

### Integration Tests

#### [`tests/entrypoints/llm/test_mm_embedding_return.py`](tests/entrypoints/llm/test_mm_embedding_return.py) (NEW FILE)
- `test_mm_embedding_return_with_llava()`: End-to-end test with LLaVA
- `test_mm_embedding_not_returned_when_disabled()`: Test default behavior
- `test_mm_embedding_with_text_only_input()`: Test text-only inputs

Note: Integration tests are marked as skipped by default (require GPU and model)

## Documentation

### User Documentation

#### [`docs/features/mm_embedding_return.md`](docs/features/mm_embedding_return.md) (NEW FILE)
- Overview of the feature
- Usage examples (LLM class and server)
- Key features and implementation details
- Memory considerations
- Example use cases
- Limitations
- Configuration
- Troubleshooting

### Example Code

#### [`examples/offline_inference/mm_embedding_example.py`](examples/offline_inference/mm_embedding_example.py) (NEW FILE)
- Complete working example
- Shows both multimodal and text-only inputs
- Demonstrates accessing and inspecting embeddings
- Includes comments and explanations

## Key Design Decisions

1. **CPU Storage**: Embeddings are stored on CPU to avoid GPU memory overhead
   - Trade-off: Slightly slower access, but better memory efficiency

2. **Request-Level Capture**: Unlike routed_experts (layer-level), mm_embedding is captured once per request
   - Simpler implementation
   - Matches the semantic meaning (one embedding per multimodal input)

3. **Singleton Pattern**: Global capturer instance for easy access across components
   - Consistent with `RoutedExpertsCapturer` design
   - Simplifies initialization and cleanup

4. **Automatic Cleanup**: Embeddings are removed when requests complete
   - Prevents memory leaks
   - No manual cleanup required

5. **Streaming Compatible**: Works with both streaming and non-streaming outputs
   - Embedding returned when request finishes
   - Compatible with existing streaming infrastructure

## Compatibility

- ✅ Works with LLM class
- ✅ Works with AsyncEngine
- ✅ Compatible with streaming outputs
- ✅ Compatible with non-streaming outputs
- ⚠️ OpenAI API endpoint does not expose mm_embedding (by design)
- ✅ Text-only inputs return `mm_embedding=None`
- ✅ Multimodal models only (text-only models return `None`)

## Performance Considerations

1. **Memory**: CPU memory usage increases with number of active requests
   - Each embedding: `(num_features, hidden_size) * 4 bytes` (float32)
   - Example: 576 features × 4096 hidden_size = ~9.4 MB per request

2. **CPU-GPU Transfer**: Minimal overhead (one-time copy to CPU per request)

3. **Computation**: No additional computation (embeddings already generated)

## Future Enhancements

Potential improvements for future versions:

1. **Selective Modality Return**: Return embeddings for specific modalities only
2. **Compression**: Optional compression for large embeddings
3. **Streaming Embeddings**: Return embeddings progressively for long sequences
4. **OpenAI API Support**: Add custom extension to expose embeddings in API
5. **Embedding Pooling**: Optional pooling strategies (mean, max, etc.)

## Files Modified

### Core Implementation
- `vllm/config/multimodal.py`
- `vllm/engine/arg_utils.py`
- `vllm/entrypoints/llm.py`
- `vllm/outputs.py`
- `vllm/v1/engine/__init__.py`
- `vllm/v1/worker/gpu_model_runner.py`
- `vllm/v1/core/sched/scheduler.py`
- `vllm/v1/engine/output_processor.py`

### New Files
- `vllm/multimodal/mm_embedding_capturer.py`
- `tests/multimodal/test_mm_embedding_capture.py`
- `tests/entrypoints/llm/test_mm_embedding_return.py`
- `docs/features/mm_embedding_return.md`
- `examples/offline_inference/mm_embedding_example.py`

## Usage Example

```python
from vllm import LLM, SamplingParams
from PIL import Image

# Initialize with feature enabled
llm = LLM(
    model="llava-hf/llava-1.5-7b-hf",
    enable_return_mm_embedding=True,
)

# Generate with multimodal input
image = Image.open("image.jpg")
outputs = llm.generate({
    "prompt": "<image>\nDescribe this image.",
    "multi_modal_data": {"image": image},
})

# Access the embedding
embedding = outputs[0].outputs[0].mm_embedding
print(f"Embedding shape: {embedding.shape}")  # (num_features, hidden_size)
```

## Conclusion

This implementation provides a clean, efficient way to extract multimodal embeddings from vLLM, following the established patterns (like `routed_experts`) while being optimized for the specific use case of multimodal models.
