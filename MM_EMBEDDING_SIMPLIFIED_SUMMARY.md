# Multimodal Embedding Return Feature - Simplified Implementation

## Overview

This feature enables vLLM to return multimodal embeddings generated by the encoder during inference. When `enable_return_mm_embedding=True`, the embeddings are captured and returned as torch CPU tensors in the `CompletionOutput`.

## Key Simplification

**No separate capturer needed!** The implementation directly uses the `mm_embeds` already available in `GPUModelRunner._preprocess()`, eliminating the need for a separate `MMEmbeddingCapturer` class.

## Implementation Details

### 1. Configuration (Same as before)

#### Files Modified:
- [`vllm/config/multimodal.py`](vllm/config/multimodal.py) - Added `enable_return_mm_embedding` parameter
- [`vllm/engine/arg_utils.py`](vllm/engine/arg_utils.py) - Added CLI argument
- [`vllm/entrypoints/llm.py`](vllm/entrypoints/llm.py) - Added LLM parameter

### 2. Data Structures

#### [`vllm/v1/worker/gpu_input_batch.py`](vllm/v1/worker/gpu_input_batch.py)
Added `mm_embedding` field to `CachedRequestState`:
```python
class CachedRequestState:
    # ... existing fields ...
    mm_embedding: "torch.Tensor | None" = None
```

#### [`vllm/v1/outputs.py`](vllm/v1/outputs.py)
- Added `mm_embedding` to `CompletionOutput`
- Added `mm_embeddings: dict[str, torch.Tensor]` to `ModelRunnerOutput`
- Added `mm_embedding` to `EngineCoreOutput`

### 3. Capture Logic (Simplified!)

#### [`vllm/v1/worker/gpu_model_runner.py`](vllm/v1/worker/gpu_model_runner.py)

**In `_preprocess()` method:**
```python
if self.supports_mm_inputs and is_first_rank and not is_encoder_decoder:
    # ... existing code to get mm_embeds ...
    mm_embeds, is_mm_embed = self._gather_mm_embeddings(scheduler_output)
    
    # NEW: Capture mm_embeds directly
    mm_config = self.vllm_config.multimodal_config
    if mm_config and mm_config.enable_return_mm_embedding and mm_embeds:
        concatenated_embeds = torch.cat(mm_embeds, dim=0).cpu()
        for req_id in self.input_batch.req_ids:
            req_state = self.requests.get(req_id)
            if req_state and req_state.mm_features:
                req_state.mm_embedding = concatenated_embeds
                break
```

**In `sample_tokens()` method:**
```python
# Collect mm_embeddings from request states
mm_embeddings = None
mm_config = self.vllm_config.multimodal_config
if mm_config and mm_config.enable_return_mm_embedding:
    mm_embeddings = {}
    for req_id in req_ids_output_copy:
        req_state = self.requests.get(req_id)
        if req_state and req_state.mm_embedding is not None:
            mm_embeddings[req_id] = req_state.mm_embedding

output = ModelRunnerOutput(
    # ... other fields ...
    mm_embeddings=mm_embeddings,
)
```

### 4. Scheduler Integration

#### [`vllm/v1/core/sched/scheduler.py`](vllm/v1/core/sched/scheduler.py)

```python
def _get_mm_embedding(
    self, request: Request, model_runner_output: ModelRunnerOutput
) -> "torch.Tensor | None":
    """Get multimodal embedding for a request if enabled."""
    mm_config = self.vllm_config.multimodal_config
    if not mm_config or not mm_config.enable_return_mm_embedding:
        return None
    
    if model_runner_output.mm_embeddings is None:
        return None
    
    return model_runner_output.mm_embeddings.get(request.request_id)
```

### 5. Output Processing (Same as before)

#### [`vllm/v1/engine/output_processor.py`](vllm/v1/engine/output_processor.py)
- Pass `mm_embedding` through the output pipeline
- Include in `CompletionOutput`

## Data Flow (Simplified)

```
User Request (with multimodal data)
    ↓
LLM.generate(enable_return_mm_embedding=True)
    ↓
GPUModelRunner._preprocess()
    ├─ _gather_mm_embeddings() → mm_embeds (already exists!)
    └─ Store mm_embeds.cpu() in req_state.mm_embedding
    ↓
GPUModelRunner.sample_tokens()
    └─ Collect mm_embeddings from request states
        └─ ModelRunnerOutput(mm_embeddings={req_id: embedding})
    ↓
Scheduler.update_from_output()
    └─ _get_mm_embedding(request, model_runner_output)
        └─ Extract from model_runner_output.mm_embeddings
    ↓
EngineCoreOutput(mm_embedding=...)
    ↓
OutputProcessor.process_outputs()
    └─ CompletionOutput(mm_embedding=...)
    ↓
User receives RequestOutput with mm_embedding
```

## Key Advantages of Simplified Approach

1. **No Extra Class**: No need for `MMEmbeddingCapturer` singleton
2. **Direct Access**: Uses `mm_embeds` already computed in `_preprocess()`
3. **Simpler Code**: Fewer files, less complexity
4. **Same Functionality**: Achieves the same result with less code
5. **Better Integration**: Follows the natural data flow

## Files Modified

### Core Implementation (8 files)
1. `vllm/config/multimodal.py` - Config parameter
2. `vllm/engine/arg_utils.py` - CLI argument
3. `vllm/entrypoints/llm.py` - LLM parameter
4. `vllm/outputs.py` - Output fields (CompletionOutput)
5. `vllm/v1/outputs.py` - ModelRunnerOutput field
6. `vllm/v1/engine/__init__.py` - EngineCoreOutput field
7. `vllm/v1/worker/gpu_input_batch.py` - CachedRequestState field
8. `vllm/v1/worker/gpu_model_runner.py` - Capture and collect logic
9. `vllm/v1/core/sched/scheduler.py` - Retrieval logic
10. `vllm/v1/engine/output_processor.py` - Output processing

### Deleted Files
- ~~`vllm/multimodal/mm_embedding_capturer.py`~~ (Not needed!)
- ~~`tests/multimodal/test_mm_embedding_capture.py`~~ (Not needed!)

### Documentation & Examples (Still valid)
- `docs/features/mm_embedding_return.md`
- `examples/offline_inference/mm_embedding_example.py`
- `tests/entrypoints/llm/test_mm_embedding_return.py`

## Usage Example (Same as before)

```python
from vllm import LLM, SamplingParams
from PIL import Image

# Initialize with feature enabled
llm = LLM(
    model="llava-hf/llava-1.5-7b-hf",
    enable_return_mm_embedding=True,
)

# Generate with multimodal input
image = Image.open("image.jpg")
outputs = llm.generate({
    "prompt": "<image>\nDescribe this image.",
    "multi_modal_data": {"image": image},
})

# Access the embedding
embedding = outputs[0].outputs[0].mm_embedding
print(f"Embedding shape: {embedding.shape}")  # (num_features, hidden_size)
print(f"Device: {embedding.device}")  # cpu
```

## Comparison: Before vs After

### Before (with MMEmbeddingCapturer)
- ❌ Separate capturer class with singleton pattern
- ❌ Shared memory management (not needed)
- ❌ Extra initialization in `initialize_kv_cache()`
- ❌ Capture in `_execute_mm_encoder()` (wrong place!)
- ❌ More complex code

### After (simplified)
- ✅ Direct use of existing `mm_embeds`
- ✅ Store in `CachedRequestState` (natural place)
- ✅ Capture in `_preprocess()` (right place!)
- ✅ Pass through `ModelRunnerOutput`
- ✅ Simpler, cleaner code

## Conclusion

The simplified implementation achieves the same functionality with significantly less code by leveraging the existing `mm_embeds` computed in `_preprocess()`. This is a much better design that follows the natural data flow in vLLM.
